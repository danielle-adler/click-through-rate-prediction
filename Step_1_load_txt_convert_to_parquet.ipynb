{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 2  \n",
    "Danielle Adler, Craig Fujii, Conor Healy, YoungKoung Kim\n",
    "\n",
    "Summer 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load txt File & Convert to Parquet\n",
    "\n",
    "Uncomment code if on the cloud or on a VM.\n",
    "\n",
    "1. Setup\n",
    "1. Load `train.txt` into Spark\n",
    "1. Convert and save data as `train.parquet`\n",
    "1. Split `train.parquet` into `training_set.parquet` and `test_set.parquet`\n",
    "1. Split `training_set.parquet` into `train` splits of 1k, 10k, 100k, 1MM & save as parquet\n",
    "1. Loading in Data - both from `gsod bucket` and local `data` folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_1_million.parquet : good\n",
      "data/train_100000.parquet : good\n",
      "data/train_10000.parquet : good\n",
      "data/train_1000.parquet : good\n",
      "gs://gsod_23456/train_1_million.parquet : MISSING!!!\n",
      "gs://gsod_23456/train_100000.parquet : MISSING!!!\n",
      "gs://gsod_23456/train_10000.parquet : MISSING!!!\n",
      "gs://gsod_23456/train_1000.parquet : MISSING!!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "list_parquets_dirs = [ 'train_1_million', 'train_100000', 'train_10000', 'train_1000']\n",
    "file_dirs = ['data/', 'gs://gsod_23456/'] # this doesn't work on Pyspark kernel; Currently deleting with the GCP commnand line\n",
    "\n",
    "for i in file_dirs:    \n",
    "    for j in list_parquets_dirs:\n",
    "        directory_name = i + j + '.parquet'\n",
    "        if os.path.exists(directory_name):\n",
    "            print(directory_name, ': good')\n",
    "        else:\n",
    "            print(directory_name, ': MISSING!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, desc, mean, isnan, when, count, isnull, rank, sum, countDistinct, avg, stddev, round\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start Spark Session - only run when running local\n",
    "# from pyspark.sql import SparkSession\n",
    "# app_name = \"final_project\"\n",
    "# master = \"local[*]\"\n",
    "# spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(app_name)\\\n",
    "#         .master(master)\\\n",
    "#         .getOrCreate()\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe68d5bcad0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `train.txt` into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.load(\"gs://gsod_23456/train.txt\",format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and save data as `train.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert main dataset to parquet for efficienty\n",
    "# train.write.parquet(\"data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.write.parquet(\"gs://gsod_23456/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split `train.parquet` into `training_set.parquet` and `test_set.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = spark.read.parquet(\"gs://gsod_23456/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set 0.8 train, 0.2 test\n",
    "splits = parquetFile.randomSplit([0.8,0.2], seed=2019)\n",
    "training_set =  splits[0]\n",
    "test_set = splits[1]\n",
    "\n",
    "# all dataframes\n",
    "type(parquetFile), type(training_set), type(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert main dataset to parquet for efficienty\n",
    "# training_set.write.parquet(\"data/training_set.parquet\")\n",
    "# test_set.write.parquet(\"data/test_set.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_set.write.parquet(\"gs://gsod_23456/data/training_set.parquet\")\n",
    "test_set.write.parquet(\"gs://gsod_23456/data/test_set.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split `training_set.parquet` into `train` splits of 1k, 10k, 100k, 1MM & save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toy sets to practice; Seed = 2019 to make the same dataset for us\n",
    "# only have to run this command once\n",
    "fraction = 1000000/48000000 \n",
    "train_1_million = training_set.sample( False, fraction, 2019) # 1 million\n",
    "train_100000 = training_set.sample( False, fraction/10, 2019) # 100,000\n",
    "train_10000 = training_set.sample( False, fraction/100, 2019) # 10,000\n",
    "train_1000 = training_set.sample( False, fraction/1000, 2019) # 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete parquet directory if exists\n",
    "list_parquets_dirs = [ 'train_1_million', 'train_100000', 'train_10000', 'train_1000']\n",
    "for i in list_parquets_dirs:\n",
    "    directory_name = 'data/' + i + '.parquet'\n",
    "    if os.path.exists(directory_name):\n",
    "        print('deleting', directory_name)\n",
    "        shutil.rmtree(directory_name)\n",
    "\n",
    "# use spark dataframe to write parquet files; this command creates a directory and the files within it\n",
    "start = time.time()\n",
    "train_1_million.write.parquet(\"gs://gsod_23456/data/train_1_million.parquet\")\n",
    "train_100000.write.parquet(\"gs://gsod_23456/data/train_100000.parquet\")\n",
    "train_10000.write.parquet(\"gs://gsod_23456/data/train_10000.parquet\")\n",
    "train_1000.write.parquet(\"gs://gsod_23456/data/train_1000.parquet\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "\n",
    "# start = time.time()\n",
    "# train_1_million.write.parquet(\"data/train_1_million.parquet\")\n",
    "# train_100000.write.parquet(\"data/train_100000.parquet\")\n",
    "# train_10000.write.parquet(\"data/train_10000.parquet\")\n",
    "# train_1000.write.parquet(\"data/train_1000.parquet\")\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Data - both from `gsod bucket` and local `data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the parquet files back into the Jupyter notebook\n",
    "start = time.time()\n",
    "train_1_million = spark.read.parquet(\"gs://gsod_23456/data/train_1_million.parquet\")\n",
    "train_100000 = spark.read.parquet(\"gs://gsod_23456/data/train_100000.parquet\")\n",
    "train_10000 = spark.read.parquet(\"gs://gsod_23456/data/train_10000.parquet\")\n",
    "train_1000 = spark.read.parquet(\"gs://gsod_23456/data/train_1000.parquet\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 3.667551040649414 seconds.\n"
     ]
    }
   ],
   "source": [
    "# read parquet file\n",
    "# will focus on 10,000 for most of our EDA\n",
    "# start = time.time()\n",
    "# train_1_million_df = spark.read.parquet(\"data/train_1_million.parquet\")\n",
    "# train_100000_df    = spark.read.parquet(\"data/train_100000.parquet\")\n",
    "# train_10000_df     = spark.read.parquet(\"data/train_10000.parquet\")\n",
    "# train_1000_df      = spark.read.parquet(\"data/train_1000.parquet\")\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
