{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are working through the engineering of all of our features and then creating parquet versions of the file for ease of use in our main notebook. The approach is described in detail in our main notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from pyspark.sql import Window, Row\n",
    "from pyspark.sql.functions import col, desc, mean, isnan, when, count, isnull, rank, sum, countDistinct, avg, stddev, round, lit, rand, broadcast, udf, log, monotonically_increasing_id\n",
    "from pyspark.sql.types import LongType, IntegerType, StringType, DoubleType, ArrayType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, OneHotEncoderEstimator, VectorAssembler, MinMaxScaler, Imputer\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa155161f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMINDER: If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 3.08327054977417 seconds.\n"
     ]
    }
   ],
   "source": [
    "# read parquet file\n",
    "# will focus on 10,000 for most of our EDA\n",
    "start = time.time()\n",
    "train_1_million_df = spark.read.parquet(\"data/train_1_million.parquet\")\n",
    "train_100000_df    = spark.read.parquet(\"data/train_100000.parquet\")\n",
    "train_10000_df     = spark.read.parquet(\"data/train_10000.parquet\")\n",
    "train_1000_df      = spark.read.parquet(\"data/train_1000.parquet\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train and test dataframe\n",
    "splits = train_1_million_df.randomSplit([0.8, 0.2], seed = 2019)\n",
    "df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_recoding(column):\n",
    "    if column > 0.6:\n",
    "        return 'H6'\n",
    "    if column > 0.1:\n",
    "        return 'H1'\n",
    "    return 'L'\n",
    "\n",
    "func_udf = udf(cat_recoding, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_as_Missing(x):\n",
    "    return when(col(x).isNotNull(), col(x)).otherwise('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_as_Missing_2(x):\n",
    "    return when(col(x).isNotNull(), col(x)).otherwise('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_files(directory_name, df, col_1, col_2):\n",
    "    start = time.time()\n",
    "    \n",
    "    # delete parquet directory if exist\n",
    "    if os.path.exists(directory_name):\n",
    "        print('\\ndeleting', directory_name)\n",
    "        shutil.rmtree(directory_name)\n",
    "\n",
    "    # First, we are creating few partition buckets\n",
    "    df.write.partitionBy(col_1, col_2).parquet(directory_name)\n",
    "    \n",
    "    print(f'... completed {directory_name} parquet file job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep = df.select(df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all columns floats and taking the log transformations\n",
    "df_num = df.select(*(log(col(i) + 1).cast(\"float\").alias(i) for i in df.columns[1:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the mean for the null values\n",
    "imputer = Imputer(strategy = \"mean\", inputCols = df_num.columns, \n",
    "                  outputCols = [i for i in df_num.columns])\n",
    "df_num_mean_fit = imputer.fit(df_num)\n",
    "df_num_mean = df_num_mean_fit.transform(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a MinMax scaler\n",
    "assembler_mean = VectorAssembler(inputCols = df_num_mean.columns, outputCol = \"features\")\n",
    "transformed_mean = assembler_mean.transform(df_num_mean)\n",
    "scaler_mean = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel_mean = scaler_mean.fit(transformed_mean.select(\"features\"))\n",
    "scaledData_mean = scalerModel_mean.transform(transformed_mean).drop(\"features\")\n",
    "df_num_mean = scaledData_mean.select(scaledData_mean.columns[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_zero = df_num.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a MinMax scaler\n",
    "assembler_zero = VectorAssembler(inputCols = df_num_zero.columns, outputCol = \"features\")\n",
    "transformed_zero = assembler_zero.transform(df_num_zero)\n",
    "scaler_zero = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel_zero = scaler_zero.fit(transformed_zero.select(\"features\"))\n",
    "scaledData_zero = scalerModel_zero.transform(transformed_zero).drop(\"features\")\n",
    "df_num_zero = scaledData_zero.select(scaledData_zero.columns[13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_dep = test_df.select(test_df.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all columns floats\n",
    "test_df_num = test_df.select(*(log(col(i)).cast(\"float\").alias(i) for i in test_df.columns[1:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the train numeric mean for the null test values\n",
    "test_df_num_mean = df_num_mean_fit.transform(test_df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the min max scaler for the test set\n",
    "assembler_mean_test = VectorAssembler(inputCols = test_df_num_mean.columns, outputCol = \"features\")\n",
    "transformed_mean_test = assembler_mean_test.transform(test_df_num_mean)\n",
    "\n",
    "scaledData_mean_test = scalerModel_mean.transform(transformed_mean_test).drop(\"features\")\n",
    "test_df_num_mean = scaledData_mean_test.select(scaledData_mean_test.columns[13:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_num_zero = test_df_num.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the min max scaler for the test set\n",
    "assembler_zero_test = VectorAssembler(inputCols = test_df_num_zero.columns, outputCol = \"features\")\n",
    "transformed_zero_test = assembler_zero_test.transform(test_df_num_zero)\n",
    "\n",
    "scaledData_zero_test = scalerModel_zero.transform(transformed_zero_test).drop(\"features\")\n",
    "test_df_num_zero = scaledData_zero_test.select(scaledData_zero_test.columns[13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Average of Dependent Variable for Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.8594028949737549 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "frames = {}\n",
    "for i in df_cat.columns[14:]:\n",
    "    recode_df = df_cat.groupby(i, '_c0').count().sort(desc('count'))\n",
    "    recode_df = recode_df.groupBy(i).agg((sum(recode_df['_c0'] * recode_df['count'])/\n",
    "                                          sum(recode_df['count'])).alias(i + \"_wv\")).sort(i)\n",
    "    frames['frame{}'.format(i)] = recode_df\n",
    "    \n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|    _c14|_c14_wv|\n",
      "+--------+-------+\n",
      "|000d72dd|   0.25|\n",
      "|003f4253|    1.0|\n",
      "|00c2e152|    0.0|\n",
      "|010e5266|    0.0|\n",
      "|0121ecd4|    0.0|\n",
      "|0158c7d2|    0.0|\n",
      "|024d3a13|    0.0|\n",
      "|0284361c|    0.5|\n",
      "|02f0e0c9|    0.0|\n",
      "|02f970ca|    0.8|\n",
      "|037aa84d|    0.0|\n",
      "|0434607f|    0.4|\n",
      "|04eb6da9|    1.0|\n",
      "|04f97c29|    0.0|\n",
      "|05048b8c|    0.0|\n",
      "|054f9f17|    0.0|\n",
      "|0550a183|    0.5|\n",
      "|058a7626|    1.0|\n",
      "|05930803|    0.0|\n",
      "|05c26cc9|    0.5|\n",
      "+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "... completed job in 6.966044902801514 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Showing one example\n",
    "start = time.time()\n",
    "frames['frame{}'.format('_c14')].show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.7008023262023926 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = df_cat.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    df_cat = df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "\n",
    "# only keeping the weighted value categories\n",
    "df_cat = df_cat.select(df_cat.columns[40:])\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 190.8936688899994 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# imputing the mean for the null values\n",
    "imputer = Imputer(strategy = \"mean\", inputCols = df_cat.columns, \n",
    "                  outputCols = [i for i in df_cat.columns])\n",
    "\n",
    "df_cat_wv_fit = imputer.fit(df_cat)\n",
    "df_cat_wv = df_cat_wv_fit.transform(df_cat)\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Average of Dependent Variable for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_cat = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.6154842376708984 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# including the weighted value training data transformations on our test data\n",
    "for i in recode_cols:\n",
    "    test_df_cat = test_df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "    \n",
    "# only keeping the weighted value categories\n",
    "test_df_cat = test_df_cat.select(test_df_cat.columns[40:])\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the train categorical mean for the null test values\n",
    "test_df_cat_wv = df_cat_wv_fit.transform(test_df_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Relative Categories to Dependent Variable for Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1.2210311889648438 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "frames = {}\n",
    "for i in df_cat.columns[14:]:\n",
    "    recode_df = df_cat.groupby(i, '_c0').count().sort(desc('count'))\n",
    "    recode_df = recode_df.groupBy(i).agg((sum(recode_df['_c0'] * recode_df['count'])/\n",
    "                                          sum(recode_df['count'])).alias(i+\"_p\")).sort(desc(i+\"_p\"))\n",
    "    recode_df = recode_df.withColumn(i+'_wgt', when(col(i+\"_p\")> 0.2, 'H').otherwise('L'))\n",
    "    frames['frame{}'.format(i)] = recode_df.select([i,i+'_wgt'])\n",
    "    \n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "\n",
    "# taking the average\n",
    "# this replaces the first and second recode_df\n",
    "# recode_df = df_cat.groupBy(i).agg((mean(df_cat['_c0'])).alias(i+\"_p\")).sort(desc(i+\"_p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    _c30|_c30_wgt|\n",
      "+--------+--------+\n",
      "|8efede7f|       H|\n",
      "|e5ba7672|       H|\n",
      "|27c07bd6|       H|\n",
      "|3486227d|       H|\n",
      "|07c540c4|       H|\n",
      "|d4bb7bd8|       L|\n",
      "|1e88c74f|       L|\n",
      "|776ce399|       L|\n",
      "|2005abd1|       L|\n",
      "|af5d780c|       L|\n",
      "+--------+--------+\n",
      "\n",
      "... completed job in 5.990954875946045 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Showing one example\n",
    "start = time.time()\n",
    "frames['frame{}'.format('_c30')].show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.5451486110687256 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = df_cat.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    df_cat = df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "    \n",
    "# only keeping the transformed categories\n",
    "df_cat_wgt_HL = df_cat.select(df_cat.columns[40:])\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change null to 'L' category\n",
    "for i in df_cat_wgt_HL.columns:\n",
    "    df_cat_wgt_HL = df_cat_wgt_HL.withColumn(i, null_as_Missing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Each Category:\n",
      "+--------+------+\n",
      "|_c14_wgt| count|\n",
      "+--------+------+\n",
      "|       H|607941|\n",
      "|       L|  2426|\n",
      "+--------+------+\n",
      "\n",
      "... completed job in 157.56113076210022 seconds.\n"
     ]
    }
   ],
   "source": [
    "# see the recoded variables\n",
    "start = time.time()\n",
    "print(\"Counts for Each Category:\")\n",
    "for i in df_cat_wgt_HL.columns[:1]: \n",
    "    df_cat_wgt_HL.groupby(i).count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Relative Categories to Dependent Variable for Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_cat = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.5343544483184814 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = test_df_cat.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    test_df_cat = test_df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "    \n",
    "# only keeping the transformed categories\n",
    "test_df_cat_wgt_HL = test_df_cat.select(test_df_cat.columns[40:])\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change null to 'L' category (same as the train set)\n",
    "for i in test_df_cat_wgt_HL.columns:\n",
    "    test_df_cat_wgt_HL = test_df_cat_wgt_HL.withColumn(i, null_as_Missing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Each Category:\n",
      "+--------+------+\n",
      "|_c14_wgt| count|\n",
      "+--------+------+\n",
      "|       H|152429|\n",
      "|       L|   644|\n",
      "+--------+------+\n",
      "\n",
      "... completed job in 163.7044665813446 seconds.\n"
     ]
    }
   ],
   "source": [
    "# see the recoded variables\n",
    "start = time.time()\n",
    "print(\"Counts for Each Category:\")\n",
    "for i in test_df_cat_wgt_HL.columns[:1]: \n",
    "    test_df_cat_wgt_HL.groupby(i).count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Relative Categories to Dependent Variable for Train with M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1.1528358459472656 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "frames = {}\n",
    "for i in df_cat.columns[14:]:\n",
    "    recode_df = df_cat.groupby(i, '_c0').count().sort(desc('count'))\n",
    "    recode_df = recode_df.groupBy(i).agg((sum(recode_df['_c0'] * recode_df['count'])/\n",
    "                                          sum(recode_df['count'])).alias(i+\"_p\")).sort(desc(i+\"_p\"))\n",
    "    recode_df = recode_df.withColumn(i+'_wgt', func_udf(col(i+\"_p\")))\n",
    "    frames['frame{}'.format(i)] = recode_df.select([i,i+'_wgt'])\n",
    "    \n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "\n",
    "# taking the average\n",
    "# this replaces the first and second recode_df\n",
    "# recode_df = df_cat.groupBy(i).agg((mean(df_cat['_c0'])).alias(i+\"_p\")).sort(desc(i+\"_p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    _c30|_c30_wgt|\n",
      "+--------+--------+\n",
      "|8efede7f|      H1|\n",
      "|e5ba7672|      H1|\n",
      "|27c07bd6|      H1|\n",
      "|3486227d|      H1|\n",
      "|07c540c4|      H1|\n",
      "|d4bb7bd8|      H1|\n",
      "|1e88c74f|      H1|\n",
      "|776ce399|      H1|\n",
      "|2005abd1|      H1|\n",
      "|af5d780c|       L|\n",
      "+--------+--------+\n",
      "\n",
      "... completed job in 5.765534400939941 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Showing one example\n",
    "start = time.time()\n",
    "frames['frame{}'.format('_c30')].show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.5545010566711426 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = df_cat.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    df_cat = df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping the transformed categories\n",
    "df_cat_wgt_HLM = df_cat.select(df_cat.columns[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change null to 'M' category\n",
    "for i in df_cat_wgt_HLM.columns:\n",
    "    df_cat_wgt_HLM = df_cat_wgt_HLM.withColumn(i, null_as_Missing_2(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Each Category:\n",
      "+--------+------+\n",
      "|_c14_wgt| count|\n",
      "+--------+------+\n",
      "|      H1|609243|\n",
      "|       L|   925|\n",
      "|      H6|   199|\n",
      "+--------+------+\n",
      "\n",
      "... completed job in 161.1223657131195 seconds.\n"
     ]
    }
   ],
   "source": [
    "# see the recoded variables\n",
    "start = time.time()\n",
    "print(\"Counts for Each Category:\")\n",
    "for i in df_cat_wgt_HLM.columns[:1]: \n",
    "    df_cat_wgt_HLM.groupby(i).count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Relative Categories to Dependent Variable for Test with M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_cat = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.49120020866394043 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = test_df_cat.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    test_df_cat = test_df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping the transformed categories\n",
    "test_df_cat_wgt_HLM = test_df_cat.select(df_cat.columns[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change null to 'L' category (same as the train set)\n",
    "for i in test_df_cat_wgt_HLM.columns:\n",
    "    test_df_cat_wgt_HLM = test_df_cat_wgt_HLM.withColumn(i, null_as_Missing_2(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for Each Category:\n",
      "+--------+------+\n",
      "|_c14_wgt| count|\n",
      "+--------+------+\n",
      "|      H1|152717|\n",
      "|       L|   226|\n",
      "|       M|    85|\n",
      "|      H6|    45|\n",
      "+--------+------+\n",
      "\n",
      "... completed job in 158.9292495250702 seconds.\n"
     ]
    }
   ],
   "source": [
    "# see the recoded variables\n",
    "start = time.time()\n",
    "print(\"Counts for Each Category:\")\n",
    "for i in test_df_cat_wgt_HLM.columns[:1]: \n",
    "    test_df_cat_wgt_HLM.groupby(i).count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add indices to each dataframe for joining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dep = df_dep.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_num_zero = df_num_zero.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_num_mean = df_num_mean.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_cat_wv = df_cat_wv.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_cat_wgt_HL = df_cat_wgt_HL.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_cat_wgt_HLM = df_cat_wgt_HLM.withColumn(\"index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.16069340705871582 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_zero_wv = df_num_zero.join(df_cat_wv, on = \"index\", how = \"left\").join(df_dep, on = \"index\", how = \"left\")\n",
    "df_mean_wv = df_num_mean.join(df_cat_wv, on = \"index\", how = \"left\").join(df_dep, on = \"index\", how = \"left\")\n",
    "df_zero_wgt_HL = df_num_zero.join(df_cat_wgt_HL, on = \"index\", how = \"left\").join(df_dep, on = \"index\", how = \"left\")\n",
    "df_mean_wgt_HL = df_num_mean.join(df_cat_wgt_HL, on = \"index\", how = \"left\").join(df_dep, on = \"index\", how = \"left\")\n",
    "df_zero_wgt_HLM = df_num_zero.join(df_cat_wgt_HLM, on = \"index\", how = \"left\").join(df_dep, on = \"index\", how = \"left\")\n",
    "df_mean_wgt_HLM = df_num_mean.join(df_cat_wgt_HLM, on = \"index\", how = \"left\").join(df_dep, on = \"index\", how = \"left\")\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_dep = test_df_dep.withColumn(\"index\", monotonically_increasing_id())\n",
    "test_df_num_zero = test_df_num_zero.withColumn(\"index\", monotonically_increasing_id())\n",
    "test_df_num_mean = test_df_num_mean.withColumn(\"index\", monotonically_increasing_id())\n",
    "test_df_cat_wv = test_df_cat_wv.withColumn(\"index\", monotonically_increasing_id())\n",
    "test_df_cat_wgt_HL = test_df_cat_wgt_HL.withColumn(\"index\", monotonically_increasing_id())\n",
    "test_df_cat_wgt_HLM = test_df_cat_wgt_HLM.withColumn(\"index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 0.12122201919555664 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "test_df_zero_wv = test_df_num_zero.join(test_df_cat_wv, on = \"index\", how = \"left\").join(test_df_dep, on = \"index\", how = \"left\")\n",
    "test_df_mean_wv = test_df_num_mean.join(test_df_cat_wv, on = \"index\", how = \"left\").join(test_df_dep, on = \"index\", how = \"left\")\n",
    "test_df_zero_wgt_HL = test_df_num_zero.join(test_df_cat_wgt_HL, on = \"index\", how = \"left\").join(test_df_dep, on = \"index\", how = \"left\")\n",
    "test_df_mean_wgt_HL = test_df_num_mean.join(test_df_cat_wgt_HL, on = \"index\", how = \"left\").join(test_df_dep, on = \"index\", how = \"left\")\n",
    "test_df_zero_wgt_HLM = test_df_num_zero.join(test_df_cat_wgt_HLM, on = \"index\", how = \"left\").join(test_df_dep, on = \"index\", how = \"left\")\n",
    "test_df_mean_wgt_HLM = test_df_num_mean.join(test_df_cat_wgt_HLM, on = \"index\", how = \"left\").join(test_df_dep, on = \"index\", how = \"left\")\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet all of the feature files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable will not be on these files because the dependent variable will always be the same and it required no featuring engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deleting data/df_zero_wv.parquet\n",
      "... completed data/df_zero_wv.parquet parquet file job in 207.8179919719696 seconds.\n",
      "\n",
      "deleting data/df_mean_wv.parquet\n",
      "... completed data/df_mean_wv.parquet parquet file job in 204.47486424446106 seconds.\n",
      "\n",
      "deleting data/df_zero_wgt_HL.parquet\n",
      "... completed data/df_zero_wgt_HL.parquet parquet file job in 181.21654510498047 seconds.\n",
      "\n",
      "deleting data/df_mean_wgt_HL.parquet\n",
      "... completed data/df_mean_wgt_HL.parquet parquet file job in 183.55237793922424 seconds.\n",
      "\n",
      "deleting data/df_zero_wgt_HLM.parquet\n",
      "... completed data/df_zero_wgt_HLM.parquet parquet file job in 202.77417302131653 seconds.\n",
      "\n",
      "deleting data/df_mean_wgt_HLM.parquet\n",
      "... completed data/df_mean_wgt_HLM.parquet parquet file job in 214.07915925979614 seconds.\n"
     ]
    }
   ],
   "source": [
    "# parquet all train files\n",
    "parquet_files(\"data/df_zero_wv.parquet\", df_zero_wv, \"_c0\", \"_c22_wv\")\n",
    "parquet_files(\"data/df_mean_wv.parquet\", df_mean_wv, \"_c0\", \"_c22_wv\")\n",
    "parquet_files(\"data/df_zero_wgt_HL.parquet\", df_zero_wgt_HL, \"_c0\", \"_c22_wgt\")\n",
    "parquet_files(\"data/df_mean_wgt_HL.parquet\", df_mean_wgt_HL, \"_c0\", \"_c22_wgt\")\n",
    "parquet_files(\"data/df_zero_wgt_HLM.parquet\", df_zero_wgt_HLM, \"_c0\", \"_c22_wgt\")\n",
    "parquet_files(\"data/df_mean_wgt_HLM.parquet\", df_mean_wgt_HLM, \"_c0\", \"_c22_wgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "deleting data/test_df_zero_wv.parquet\n",
      "... completed data/test_df_zero_wv.parquet parquet file job in 198.25913763046265 seconds.\n",
      "\n",
      "deleting data/test_df_mean_wv.parquet\n",
      "... completed data/test_df_mean_wv.parquet parquet file job in 198.1466989517212 seconds.\n",
      "\n",
      "deleting data/test_df_zero_wgt_HL.parquet\n",
      "... completed data/test_df_zero_wgt_HL.parquet parquet file job in 176.74510288238525 seconds.\n",
      "\n",
      "deleting data/test_df_mean_wgt_HL.parquet\n",
      "... completed data/test_df_mean_wgt_HL.parquet parquet file job in 176.47251987457275 seconds.\n",
      "\n",
      "deleting data/test_df_zero_wgt_HLM.parquet\n",
      "... completed data/test_df_zero_wgt_HLM.parquet parquet file job in 195.98881149291992 seconds.\n",
      "\n",
      "deleting data/test_df_mean_wgt_HLM.parquet\n",
      "... completed data/test_df_mean_wgt_HLM.parquet parquet file job in 198.45913743972778 seconds.\n"
     ]
    }
   ],
   "source": [
    "# parquet all test files\n",
    "parquet_files(\"data/test_df_zero_wv.parquet\", test_df_zero_wv, \"_c0\", \"_c22_wv\")\n",
    "parquet_files(\"data/test_df_mean_wv.parquet\", test_df_mean_wv, \"_c0\", \"_c22_wv\")\n",
    "parquet_files(\"data/test_df_zero_wgt_HL.parquet\", test_df_zero_wgt_HL, \"_c0\", \"_c22_wgt\")\n",
    "parquet_files(\"data/test_df_mean_wgt_HL.parquet\", test_df_mean_wgt_HL, \"_c0\", \"_c22_wgt\")\n",
    "parquet_files(\"data/test_df_zero_wgt_HLM.parquet\", test_df_zero_wgt_HLM, \"_c0\", \"_c22_wgt\")\n",
    "parquet_files(\"data/test_df_mean_wgt_HLM.parquet\", test_df_mean_wgt_HLM, \"_c0\", \"_c22_wgt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
