{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team 2  \n",
    "Danielle Adler, Craig Fujii, Conor Healy, YoungKoung Kim\n",
    "\n",
    "Summer 2019, section [Your section numbers>]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from pyspark.sql import Window, Row\n",
    "from pyspark.sql.functions import col, desc, mean, isnan, when, count, isnull, rank, sum, countDistinct, avg, stddev, round, lit, rand, broadcast, udf, log, monotonically_increasing_id\n",
    "from pyspark.sql.types import LongType, IntegerType, StringType, DoubleType, ArrayType, FloatType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, OneHotEncoderEstimator, VectorAssembler, MinMaxScaler, Imputer\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-4b27-11-m.us-west2-b.c.ethereal-atlas-253605.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4dc0bd8d68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # start Spark Session\n",
    "# from pyspark.sql import SparkSession\n",
    "# app_name = \"final_project\"\n",
    "# master = \"local[*]\"\n",
    "# spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(app_name)\\\n",
    "#         .master(master)\\\n",
    "#         .getOrCreate()\n",
    "# sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(col_name):\n",
    "    \"\"\"Calculates key metrics on a binary column of 0s and 1s\n",
    "    Input: Column Name\n",
    "    Output: TP, FP, TN, FP, accuracy, precision, recall, F1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    TP = train_set.filter(col('_c0')==0).filter(col(col_name)==0).count()\n",
    "    FP = train_set.filter(col('_c0')==1).filter(col(col_name)==0).count()\n",
    "    TN = train_set.filter(col('_c0')==1).filter(col(col_name)==1).count()\n",
    "    FN = train_set.filter(col('_c0')==0).filter(col(col_name)==1).count()\n",
    "    accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    F1 = 2*(precision*recall)/(precision+recall)\n",
    "    print(\"True Positives: {} \\nFalse Positives: {} \\nTrue Negatives: {} \\nFalse Negatives: {} \\nAccuracy: {} \\nPrecision: {} \\nRecall: {} \\nF1 Score: {}\".format(TP, FP, TN, FN, accuracy, precision, recall, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_prep (input_df):\n",
    "    \"\"\"Parses out types of features within a dataframe\n",
    "    Input: Dataframe\n",
    "    Output: Dataframe, Categorical Column List, Numeric Column List\n",
    "    \"\"\"\n",
    "    \n",
    "    # develop feature set\n",
    "    dep_var = ['_c0']\n",
    "\n",
    "    model_df = input_df.select(input_df.columns[1:])\n",
    "\n",
    "    feature_df = model_df.select([column for column in model_df.columns \n",
    "                              if column not in dep_var])\n",
    "    \n",
    "    # numerical and categorical column split\n",
    "    cat_cols = feature_df.columns[1:]\n",
    "    num_cols = feature_df.columns[0]\n",
    "    \n",
    "    return model_df, cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_vector_maker(df, columns, lookup_df, suffix):\n",
    "    \"\"\"One-hot encodes categorical variables\n",
    "    Input: Dataframe, Categorical Column List, Lookup Dataframe, Suffix for New Columns\n",
    "    Output: Dataframe, Categorical Column List, Numeric Column List\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the lookup table vectors for each category\n",
    "    # we are doing a map-side join\n",
    "    for i in columns:\n",
    "        new_col = str(i) + str(suffix)\n",
    "        df = df.join(lookup_df, df[i]==lookup_df['id'], \"left_outer\" )\n",
    "        df = df.withColumnRenamed(\"category\", new_col) \n",
    "        lst = df.columns\n",
    "        cols = [i for i in lst if i not in ['id']]\n",
    "        df = df.select(cols)\n",
    "        \n",
    "    df = df.select([c for c in df.columns if c not in columns])\n",
    "    \n",
    "    cat_cols = df.columns[2:]\n",
    "    num_cols = df.columns[0]\n",
    "    \n",
    "    return df, cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_prep (object_type):\n",
    "    \"\"\"Assembles a dataframe of label and feature fectors for machine learning algorithm\n",
    "    Input: Dataframe, Categorical Column List, Numeric Column List\n",
    "    Ouput: Dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    model_df = object_type[0]\n",
    "    cat_cols = object_type[1]\n",
    "    num_cols = object_type[2]\n",
    "\n",
    "    # developing the string indexer and vector assembler of input and output columns\n",
    "    label_stringIdx = StringIndexer(inputCol = '_c0', outputCol = 'label')\n",
    "\n",
    "    assemblerInputs = cat_cols + [num_cols]\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    \n",
    "    # pipeline work for more setup\n",
    "    pipeline = Pipeline(stages = [label_stringIdx, assembler])\n",
    "\n",
    "    pipelineModel = pipeline.fit(model_df)\n",
    "    model_df = pipelineModel.transform(model_df)\n",
    "\n",
    "    selectedCols = ['label', 'features'] \n",
    "    model_df = model_df.select(selectedCols)\n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cross_validate (lr_model):\n",
    "    \"\"\"Hypertunes a logistic regression model based on a set of parameters\n",
    "    Input: Logistic regression classification\n",
    "    Output: Hypertuned logistic regression classification\n",
    "    \"\"\"\n",
    "\n",
    "    # for elasticNetParam, 0 is L2 and 1 is L1\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr_model.elasticNetParam, [0.0, 0.25, 0.75, 1.0]) \\\n",
    "        .addGrid(lr_model.regParam, [0.0, 0.01, 0.1, 0.5, 0.75, 1.0, 1.25]) \\\n",
    "        .build()\n",
    "\n",
    "    # optimizing for an f1 score\n",
    "    crossval = CrossValidator(estimator = lr, estimatorParamMaps = paramGrid,\n",
    "                              evaluator = MulticlassClassificationEvaluator(\n",
    "                                  labelCol = \"label\", predictionCol=\"prediction\", \n",
    "                                  metricName=\"f1\"), numFolds=3)\n",
    "    return crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_cross_validate (dt_model):\n",
    "    \"\"\"Hypertunes a decision tree model based on a set of parameters\n",
    "    Input: Decision tree classification\n",
    "    Output: Hypertuned decision tree classification\n",
    "    \"\"\"\n",
    "\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(dt_model.maxBins, [28, 30, 32, 34, 36]) \\\n",
    "        .addGrid(dt_model.maxDepth, [3, 4, 5, 6, 7]) \\\n",
    "        .addGrid(dt_model.impurity, ['gini', 'entropy']) \\\n",
    "        .build()\n",
    "\n",
    "    # optimizing for an f1 score\n",
    "    crossval = CrossValidator(estimator = dt, estimatorParamMaps = paramGrid,\n",
    "                              evaluator = MulticlassClassificationEvaluator(\n",
    "                                  labelCol = \"label\", predictionCol=\"prediction\", \n",
    "                                  metricName=\"f1\"), numFolds=3)\n",
    "    return crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_results (trainingSummary):\n",
    "    \"\"\"Showcases key metrics on logistic regression training summary\n",
    "    Input: Logistic regression training model\n",
    "    Output: Training set accuracy, precision, recall, F1 score\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting on the training set\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    accuracy = trainingSummary.accuracy\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "    f1 = trainingSummary.weightedFMeasure()\n",
    "\n",
    "    print(\"False Positive Rate: %s\\nTrue Positive Rate: %s\\nAccuracy: %s\\nPrecision: %s\\nRecall: %s\\nF1 Score: %s\"\n",
    "          % (falsePositiveRate, truePositiveRate, accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_results (predictions):\n",
    "    \"\"\"Showcases key metrics on all algorithm test datasets\n",
    "    Input: Classification training model\n",
    "    Output: Testing set accuracy, precision, recall, F1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    print(\"Accuracy:\", evaluator.evaluate(predictions))\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    print(\"Precision:\", evaluator.evaluate(predictions))\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    print(\"Recall:\", evaluator.evaluate(predictions))\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "    print(\"F1 Score:\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array(col):\n",
    "    \"\"\"Separating a vector into distinct columns\n",
    "    Input: Vector column\n",
    "    Output: Separate array columns of vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # separating a vector into its array types\n",
    "    def to_array_(v):\n",
    "        return v.toArray().tolist()\n",
    "    return udf(to_array_, ArrayType(DoubleType())).asNondeterministic()(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_as_Missing(x):\n",
    "    return when(col(x).isNotNull(), col(x)).otherwise('L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1.5045206546783447 seconds.\n"
     ]
    }
   ],
   "source": [
    "# read parquet files of features\n",
    "start = time.time()\n",
    "\n",
    "folder_hl = 'data_good'\n",
    "# mean_wgt_HL_df = spark.read.parquet(\"gs://gsod_23456/\"+folder_hl+\"/df_mean_wgt_HL.parquet\")\n",
    "# mean_wgt_HL_test_df = spark.read.parquet(\"gs://gsod_23456/\"+folder_hl+\"/test_df_mean_wgt_HL.parquet\")\n",
    "\n",
    "zero_wgt_HL_df = spark.read.parquet(\"gs://gsod_23456/\"+folder_hl+\"/df_zero_wgt_HL.parquet\")\n",
    "zero_wgt_HL_test_df = spark.read.parquet(\"gs://gsod_23456/\"+folder_hl+\"/test_df_zero_wgt_HL.parquet\")\n",
    "\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36670740, 36671468)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_wgt_HL_df.count(), mean_wgt_HL_test_df.count() , zero_wgt_HL_df.count(), zero_wgt_HL_test_df.count()\n",
    "zero_wgt_HL_df.count(), zero_wgt_HL_test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|_c14_wgt|   count|\n",
      "+--------+--------+\n",
      "|    null|30152742|\n",
      "|       H| 6516116|\n",
      "|       L|    2610|\n",
      "+--------+--------+\n",
      "\n",
      "... completed job in 20.613758087158203 seconds.\n"
     ]
    }
   ],
   "source": [
    "# # see the recoded variables\n",
    "# start = time.time()\n",
    "# print(\"Counts for Each Category:\")\n",
    "# for i in zero_wgt_HL_df.columns[:1]: \n",
    "zero_wgt_HL_df.groupby('_c14_wgt').count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|_c14_wgt|  count|\n",
      "+--------+-------+\n",
      "|    null|7539232|\n",
      "|       H|1629274|\n",
      "|       L|    643|\n",
      "+--------+-------+\n",
      "\n",
      "... completed job in 24.47908902168274 seconds.\n"
     ]
    }
   ],
   "source": [
    "# # see the recoded variables\n",
    "# start = time.time()\n",
    "# print(\"Counts for Each Category:\")\n",
    "# for i in zero_wgt_HL_df.columns[:1]: \n",
    "zero_wgt_HL_test_df.groupby('_c14_wgt').count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nulls to L for variable columns\n",
    "col = ['_c14_wgt', '_c15_wgt', '_c16_wgt', '_c17_wgt', '_c18_wgt', '_c19_wgt', '_c20_wgt', '_c21_wgt', '_c23_wgt', '_c24_wgt', '_c25_wgt', '_c26_wgt', '_c27_wgt', '_c28_wgt', '_c29_wgt', '_c30_wgt', '_c31_wgt', '_c32_wgt', '_c33_wgt', '_c34_wgt', '_c35_wgt', '_c36_wgt', '_c37_wgt', '_c38_wgt', '_c39_wgt', '_c22_wgt']\n",
    "zero_wgt_HL_df = zero_wgt_HL_df.fillna('L')\n",
    "zero_wgt_HL_test_df = zero_wgt_HL_test_df.fillna('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|_c14_wgt|   count|\n",
      "+--------+--------+\n",
      "|       L|30155352|\n",
      "|       H| 6516116|\n",
      "+--------+--------+\n",
      "\n",
      "... completed job in 207.72215056419373 seconds.\n",
      "+--------+-------+\n",
      "|_c14_wgt|  count|\n",
      "+--------+-------+\n",
      "|       L|7539875|\n",
      "|       H|1629274|\n",
      "+--------+-------+\n",
      "\n",
      "... completed job in 212.78889298439026 seconds.\n"
     ]
    }
   ],
   "source": [
    "# confirm \n",
    "zero_wgt_HL_df.groupby('_c14_wgt').count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "zero_wgt_HL_test_df.groupby('_c14_wgt').count().sort(desc('count')).show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# zero_wgt_HL_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting the lookup dataframe as we have to use it multiple times\n",
    "lookup_df_HL = broadcast(spark.createDataFrame(\n",
    "    [('H',  Vectors.dense(1.0)), \n",
    "     ('L', Vectors.dense(0.0))],\n",
    "    [\"id\", \"category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # broadcasting the lookup dataframe as we have to use it multiple times\n",
    "# lookup_df_HLM = broadcast(spark.createDataFrame(\n",
    "#     [('H6', Vectors.dense(1.0,0.0,0.0)), \n",
    "#      ('H1', Vectors.dense(0.0,1.0,0.0)),\n",
    "#      ('L',  Vectors.dense(0.0,0.0,1.0)),\n",
    "#      ('M',  Vectors.dense(0.0,0.0,0.0))],\n",
    "#     [\"id\", \"category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"label\", \n",
    "                                              predictionCol=\"prediction\", \n",
    "                                              metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center>Algorithm Modeling Matrix</center>**\n",
    "\n",
    "|\t 3 Transformation Types\t|\t2 Imputing Methods\t|\t2 Algorithms\t|\t2 Model Runs\t|\n",
    "|\t---\t|\t---\t|\t---\t|\t---\t|\n",
    "|\tWeighted Value\t|\tNulls => Mean\t|\tLogistic Regression\t|\tDefault\t|\n",
    "|\tHi Low\t|\tNulls => 0\t|\tDecision Tree\t|\tHypertuned\t|\n",
    "|\tHi Mid Low Missing\t|\t\t|\t\t|\t\t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recategorized HL with Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "# lrModel = lr.fit(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "# trainingSummary = lrModel.summary\n",
    "# print(\"Train Results:\")\n",
    "# train_results(trainingSummary)\n",
    "\n",
    "# print(\"\\nTest Results:\")\n",
    "# predictions = lrModel.transform(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_test_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "# test_results(predictions)\n",
    "\n",
    "# f1_scores['lr_mean_wgt_HL'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameter tuned\n",
    "# start = time.time()\n",
    "\n",
    "# lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "# lrModel = lr_cross_validate(lr).fit(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "\n",
    "# print(\"Test Results:\")\n",
    "# predictions = lrModel.transform(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_test_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "# test_results(predictions)\n",
    "\n",
    "# f1_scores['lr_mean_wgt_HL_tuned'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recategorized HL with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Results:\n",
      "False Positive Rate: 0.6180518768258426\n",
      "True Positive Rate: 0.7566615549723834\n",
      "Accuracy: 0.7566615549723834\n",
      "Precision: 0.7228676693477389\n",
      "Recall: 0.7566615549723834\n",
      "F1 Score: 0.7067075343618696\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.397585206653311\n",
      "Precision: 0.5939620904393667\n",
      "Recall: 0.397585206653311\n",
      "F1 Score: 0.41578348580869606\n",
      "... completed job in 663.4937987327576 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"Train Results:\")\n",
    "train_results(trainingSummary)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "predictions = lrModel.transform(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_test_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "test_results(predictions)\n",
    "\n",
    "f1_scores['lr_zero_wgt_HL'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.397585206653311\n",
      "Precision: 0.5939620904393667\n",
      "Recall: 0.397585206653311\n",
      "F1 Score: 0.41578348580869606\n",
      "... completed job in 1869.2647018432617 seconds.\n"
     ]
    }
   ],
   "source": [
    "# parameter tuned\n",
    "start = time.time()\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr_cross_validate(lr).fit(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "\n",
    "print(\"Test Results:\")\n",
    "predictions = lrModel.transform(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_test_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "test_results(predictions)\n",
    "\n",
    "f1_scores['lr_zero_wgt_HL_tuned'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recategorized with Mean HL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "# dtModel = dt.fit(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "\n",
    "# print(\"Test Results:\")\n",
    "# predictions = dtModel.transform(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_test_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "# test_results(predictions)\n",
    "\n",
    "# f1_scores['dt_mean_wgt_HL'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameter tuned\n",
    "# start = time.time()\n",
    "\n",
    "# dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "# dtModel = dt_cross_validate(dt).fit(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "\n",
    "# print(\"Test Results:\")\n",
    "# predictions = dtModel.transform(pipeline_prep(column_vector_maker(feature_prep(mean_wgt_HL_test_df)[0], \n",
    "#                                   feature_prep(mean_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "# test_results(predictions)\n",
    "\n",
    "# f1_scores['dt_mean_wgt_HL_tuned'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recategorized with Zero HL & HLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.7533489749157746\n",
      "Precision: 0.7173419093907185\n",
      "Recall: 0.7533489749157747\n",
      "F1 Score: 0.7095760914695812\n",
      "... completed job in 885.2794954776764 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "\n",
    "print(\"Test Results:\")\n",
    "predictions = dtModel.transform(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_test_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "test_results(predictions)\n",
    "\n",
    "f1_scores['dt_zero_wgt_HL'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.7639955463696795\n",
      "Precision: 0.735735533899509\n",
      "Recall: 0.7639955463696795\n",
      "F1 Score: 0.723051933997398\n",
      "... completed job in 5283.550374507904 seconds.\n"
     ]
    }
   ],
   "source": [
    "# parameter tuned\n",
    "start = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt_cross_validate(dt).fit(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_df)[1], lookup_df_HL, \"HL\")))\n",
    "\n",
    "print(\"Test Results:\")\n",
    "predictions = dtModel.transform(pipeline_prep(column_vector_maker(feature_prep(zero_wgt_HL_test_df)[0], \n",
    "                                  feature_prep(zero_wgt_HL_test_df)[1], lookup_df_HL, \"HL\")))\n",
    "test_results(predictions)\n",
    "\n",
    "f1_scores['dt_zero_wgt_HL_tuned'] = format(evaluator.evaluate(predictions), '.6f')\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... master completed job in 11279.086911439896 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(f'... master completed job in {time.time() - master_start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
