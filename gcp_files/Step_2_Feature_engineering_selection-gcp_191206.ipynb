{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prep data / Feature Creation\n",
    "1. Recode the Categorical Variable - weighted (Danielle's method) (Numeric Variables + Danielle's Features)\n",
    "    1. Load `train_1000_toy.parquet` for further use \n",
    "1. Recode the Categorical Variable - weighted (YoungKoungs's method) (Numeric Variables + YoungKoung's Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* purpose of this notebook is to create a toy data set with exploratory features that can then be save as a parquet file and read in for use for exploratory model building. \n",
    "\n",
    "* Run once start to finish to create the toy dataset parquet file. This code is not optimized and is a bit brittle. \n",
    "\n",
    "* Had to create new parquet files. Overwriting had some problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import shutil\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "from pyspark.sql import Window, Row\n",
    "from pyspark.sql.functions import col, desc, mean, isnan, when, count, isnull, rank, sum, countDistinct, avg, stddev, round, lit, rand, monotonically_increasing_id\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler, Imputer\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-4b27-m.c.skillful-flow-256319.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f95de8d2d68>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"final_project\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .config(\"spark.jars\", \"/path/to/gcs-connector-hadoop2-latest.jar\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start Spark Session\n",
    "# from pyspark.sql import SparkSession\n",
    "# app_name = \"final_project\"\n",
    "# master = \"local[*]\"\n",
    "# spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(app_name)\\\n",
    "#         .master(master)\\\n",
    "#         .getOrCreate()\n",
    "# sc = spark.sparkContext\n",
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REMINDER: If you are running this notebook on the course docker container, you can monitor the progress of your jobs using the Spark UI at: http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 3.145395517349243 seconds.\n"
     ]
    }
   ],
   "source": [
    "# read parquet file\n",
    "# will focus on 10,000 for most of our EDA\n",
    "start = time.time()\n",
    "train_10_million_df = spark.read.parquet(\"gs://gsod_23456/data/train_10_million.parquet\")\n",
    "train_5_million_df = spark.read.parquet(\"gs://gsod_23456/data/train_5_million.parquet\")\n",
    "train_1_million_df = spark.read.parquet(\"gs://gsod_23456/data/train_1_million.parquet\")\n",
    "train_100000_df    = spark.read.parquet(\"gs://gsod_23456/data/train_100000.parquet\")\n",
    "train_10000_df     = spark.read.parquet(\"gs://gsod_23456/data/train_10000.parquet\")\n",
    "train_1000_df      = spark.read.parquet(\"gs://gsod_23456/data/train_1000.parquet\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 8.75810194015503 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read parquet file\n",
    "# # will focus on 10,000 for most of our EDA\n",
    "# start = time.time()\n",
    "# train_1_million_df = spark.read.parquet(\"data/train_1_million.parquet\")\n",
    "# train_100000_df    = spark.read.parquet(\"data/train_100000.parquet\")\n",
    "# train_10000_df     = spark.read.parquet(\"data/train_10000.parquet\")\n",
    "# train_1000_df      = spark.read.parquet(\"data/train_1000.parquet\")\n",
    "# print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The copy is not mutable\n",
    "df = spark.read.parquet(\"gs://gsod_23456/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c1=None, _c2=139, _c3=None, _c4=2, _c5=4828, _c6=28, _c7=11, _c8=2, _c9=28, _c10=None, _c11=1, _c12=0, _c13=2, _c14='05db9164', _c15='d833535f', _c16='77f2f2e5', _c17='d16679b9', _c18='4cf72387', _c19='fe6b92e5', _c20='9ea2e0f0', _c21='0b153874', _c23='43b7a3fa', _c24='2dad6ba2', _c25='9f32b866', _c26='47cb697a', _c27='07d13a8f', _c28='943169c2', _c29='31ca40b6', _c30='e5ba7672', _c31='281769c2', _c32=None, _c33=None, _c34='dfcfc3fa', _c35='c9d4222a', _c36='32c7478e', _c37='aee52b6f', _c38=None, _c39=None, _c0=0, _c22='a73ee510')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all columns floats\n",
    "df_num = df.select(*(col(i).cast(\"float\").alias(i) for i in df.columns[1:14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the mean for the null values\n",
    "imputer = Imputer(strategy = \"mean\", inputCols = df_num.columns, \n",
    "                  outputCols = [i for i in df_num.columns])\n",
    "df_num_mean = imputer.fit(df_num).transform(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_zero = df_num.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Average of Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1.1267173290252686 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "frames = {}\n",
    "for i in df_cat.columns[14:]:\n",
    "    recode_df = df_cat.groupby(i, '_c0').count().sort(desc('count'))\n",
    "    recode_df_2 = recode_df.groupBy(i).agg((sum(recode_df['_c0'] * recode_df['count'])/sum(recode_df['count'])).alias(i + \"_wv\")).sort(i)\n",
    "    frames['frame{}'.format(i)] = recode_df_2\n",
    "    \n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 1.5797109603881836 seconds. 10000\n",
    "# ... completed job in 1.22529935836792 seconds.  100000\n",
    "# ... completed job in 1.1574714183807373 seconds. 1mil\n",
    "# ... completed job in 1.1070666313171387 seconds. 5 mil\n",
    "# ... completed job in 1.5035927295684814 seconds. 10 mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|    _c15|            _c15_wv|\n",
      "+--------+-------------------+\n",
      "|00ac063c|0.16605466628739882|\n",
      "|014e4174| 0.3184774753071549|\n",
      "|016cbb4f|0.15769017212659633|\n",
      "|01d108a8|0.38713910761154857|\n",
      "|023a27f8|0.22647420834732013|\n",
      "|028bd518|0.12851043643263757|\n",
      "|02de4366|  0.391008174386921|\n",
      "|0363d860|0.24634070383058237|\n",
      "|038ac0e2| 0.3299492385786802|\n",
      "|04236da6| 0.2510460251046025|\n",
      "|04440d29|0.17782948313112848|\n",
      "|0468d672|0.15128299330862008|\n",
      "|04e09220|0.41324485098325126|\n",
      "|051a26e5|0.32195599758890897|\n",
      "|053c35c0| 0.2504708097928437|\n",
      "|06174070| 0.2680912712785223|\n",
      "|064c8f31|0.31504843079205247|\n",
      "|069b6d24|0.16120644825793032|\n",
      "|076c38e2|0.17625899280575538|\n",
      "|083aa75b|0.10343163862251975|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "... completed job in 6.468371868133545 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Showing one example\n",
    "start = time.time()\n",
    "frames['frame{}'.format('_c15')].show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 4.743180513381958 seconds. 10000\n",
    "# ... completed job in 1.9786813259124756 seconds.100000\n",
    "# ... completed job in 3.5676050186157227 seconds. 1 mil\n",
    "# ... completed job in 3.299978494644165 seconds. 5 mil\n",
    "# ... completed job in 3.268929958343506 seconds. 10 mil\n",
    "# ... completed job in 6.468371868133545 seconds. full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 3.7436633110046387 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = df_cat.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    df_cat = df_cat.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 4.176668167114258 seconds. 10000\n",
    "# ... completed job in 3.8941476345062256 seconds.100000\n",
    "# ... completed job in 3.8023264408111572 seconds. 1 mi\n",
    "# ... completed job in 3.7413086891174316 seconds. 5 mil\n",
    "# ... completed job in 3.768989086151123 seconds. 10 mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping the weighted value categories\n",
    "df_cat_pre = df_cat.select(df_cat.columns[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1770.5356452465057 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# imputing the mean for the null values\n",
    "imputer = Imputer(strategy = \"mean\", inputCols = df_cat_pre.columns, \n",
    "                  outputCols = [i for i in df_cat_pre.columns])\n",
    "df_cat_wv = imputer.fit(df_cat_pre).transform(df_cat_pre)\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 37.89948034286499 seconds. 10000\n",
    "# ... completed job in 32.890904664993286 seconds.100000\n",
    "# ... completed job in 38.56995725631714 seconds. 1 mil\n",
    "# ... completed job in 317.81101274490356 seconds. 5 mil\n",
    "# ... completed job in 422.4889533519745 seconds. 10 mil \n",
    "# ... completed job in 1770.5356452465057 seconds. 29.5 minutes for full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode Categories as Average of Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1.378826379776001 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "frames = {}\n",
    "for i in df_cat_2.columns[14:]:         \n",
    "    recode_df = df_cat_2.groupBy(i).agg((mean(df_cat_2['_c0'])).alias(i+\"_p\")).sort(desc(i+\"_p\"))    \n",
    "    recode_df = recode_df.withColumn(i+'_wgt', when(col(i+\"_p\")> 0.2, 'H').otherwise('L'))\n",
    "    frames['frame{}'.format(i)] = recode_df.select([i,i+'_wgt'])\n",
    "    \n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 1.0335583686828613 seconds. 10000\n",
    "# ... completed job in 0.9747071266174316 seconds. 100000\n",
    "# ... completed job in 0.9798829555511475 seconds. 1 mil\n",
    "# ... completed job in 1.001121997833252 seconds.  5 mil\n",
    "# ... completed job in 1.0045771598815918 seconds. 10 mil\n",
    "# ... completed job in 1.378826379776001 seconds. full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    _c30|_c30_wgt|\n",
      "+--------+--------+\n",
      "|8efede7f|       H|\n",
      "|e5ba7672|       H|\n",
      "|27c07bd6|       H|\n",
      "|3486227d|       H|\n",
      "|07c540c4|       H|\n",
      "|d4bb7bd8|       L|\n",
      "|1e88c74f|       L|\n",
      "|776ce399|       L|\n",
      "|2005abd1|       L|\n",
      "|af5d780c|       L|\n",
      "+--------+--------+\n",
      "\n",
      "... completed job in 12.459106683731079 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Showing one example\n",
    "start = time.time()\n",
    "frames['frame{}'.format('_c30')].show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 3.0513346195220947 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# The columns will get longer and longer without this specification\n",
    "recode_cols = df_cat_2.columns[14:]\n",
    "\n",
    "for i in recode_cols:\n",
    "    df_cat_2 = df_cat_2.join(frames['frame{}'.format(i)], on = i, how = \"left\")\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 2.4024016857147217 seconds. 10000\n",
    "# ... completed job in 2.331226348876953 seconds.  100000\n",
    "# ... completed job in 2.632216215133667 seconds. 5 mil\n",
    "# ... completed job in 2.607677459716797 seconds. 10 mil\n",
    "# ... completed job in 3.0513346195220947 seconds. full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keeping the transformed categories\n",
    "df_cat_wgt = df_cat_2.select(df_cat_2.columns[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change null to 'M' category\n",
    "def null_as_Missing(x):\n",
    "    return when(col(x).isNotNull(), col(x)).otherwise('M')\n",
    "\n",
    "for i in df_cat_wgt.columns[14:]:\n",
    "    df_cat_wgt = df_cat_wgt.withColumn(i, null_as_Missing(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add indices to each dataframe for sorting (come back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num_zero = df_num_zero.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_num_mean = df_num_mean.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_cat_wv = df_cat_wv.withColumn(\"index\", monotonically_increasing_id())\n",
    "df_cat_wgt = df_cat_wgt.withColumn(\"index\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 1.0341036319732666 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_zero_wv = df_num_zero.join(df_cat_wv, on = \"index\", how = \"left\")\n",
    "df_mean_wv = df_num_mean.join(df_cat_wv, on = \"index\", how = \"left\")\n",
    "df_zero_wgt = df_num_zero.join(df_cat_wgt, on = \"index\", how = \"left\")\n",
    "df_mean_wgt = df_num_mean.join(df_cat_wgt, on = \"index\", how = \"left\")\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 1.1673033237457275 seconds. 10000\n",
    "# ... completed job in 1.0087032318115234 seconds. 100000\n",
    "# .. completed job in 0.986668586730957 seconds. 1 mil\n",
    "# ... completed job in 1.0337064266204834 seconds. 5 mil\n",
    "#  ... completed job in 1.0097198486328125 seconds. 10 mil\n",
    "# ... completed job in 1.0341036319732666 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet all of the feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 67.96506094932556 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "directory_name=\"gs://gsod_23456/data/df_zero_wv_full.parquet\"\n",
    "\n",
    "# delete parquet directory if exists\n",
    "if os.path.exists(directory_name):\n",
    "    print('deleting', directory_name)\n",
    "    shutil.rmtree(directory_name)\n",
    "\n",
    "# Write `train_1000_toy.parquet` for posterity\n",
    "# First, we are creating few partition buckets\n",
    "df.write.partitionBy(\"_c0\", \"_c22\").parquet(directory_name)\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 3.9202239513397217 seconds. 10000\n",
    "# ... completed job in 3.2623648643493652 seconds. 100000\n",
    "# ... completed job in 15.71683955192566 seconds. 1 mil\n",
    "# ... completed job in 34.01933479309082 seconds. 5 mil\n",
    "# ... completed job in 44.703779220581055 seconds. 10 mil\n",
    "# ... completed job in 67.96506094932556 seconds full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 42.87833905220032 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "directory_name=\"gs://gsod_23456/data/df_mean_wv_full.parquet\"\n",
    "\n",
    "# delete parquet directory if exists\n",
    "if os.path.exists(directory_name):\n",
    "    print('deleting', directory_name)\n",
    "    shutil.rmtree(directory_name)\n",
    "\n",
    "# Write `train_1000_toy.parquet` for posterity\n",
    "# First, we are creating few partition buckets\n",
    "df.write.partitionBy(\"_c0\", \"_c22\").parquet(directory_name)\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 2.379305601119995 seconds. 10000\n",
    "# ... completed job in 9.892587184906006 seconds. 100000\n",
    "# ... completed job in 13.134225845336914 seconds.1 mil\n",
    "# ... completed job in 29.67958402633667 seconds. 5 mil\n",
    "# ... completed job in 29.74158763885498 seconds. 10 mil\n",
    "# ... completed job in 42.87833905220032 seconds. fulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 44.6361300945282 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "directory_name=\"gs://gsod_23456/data/df_zero_wgt_full.parquet\"\n",
    "\n",
    "# delete parquet directory if exists\n",
    "if os.path.exists(directory_name):\n",
    "    print('deleting', directory_name)\n",
    "    shutil.rmtree(directory_name)\n",
    "\n",
    "# Write `train_1000_toy.parquet` for posterity\n",
    "# First, we are creating few partition buckets\n",
    "df.write.partitionBy(\"_c0\", \"_c22\").parquet(directory_name)\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 1.602597713470459 seconds 10000.\n",
    "# ... completed job in 15.576507091522217 seconds100000\n",
    "# ... completed job in 9.965850353240967 seconds. 1 mil\n",
    "# ... completed job in 25.147286891937256 seconds. 5 mil\n",
    "# ... completed job in 24.53834342956543 seconds.10 mil\n",
    "# ... completed job in 44.6361300945282 seconds. full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... completed job in 41.63871169090271 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "directory_name=\"gs://gsod_23456/data/df_mean_wgt_full.parquet\"\n",
    "\n",
    "# delete parquet directory if exists\n",
    "if os.path.exists(directory_name):\n",
    "    print('deleting', directory_name)\n",
    "    shutil.rmtree(directory_name)\n",
    "\n",
    "# Write `train_1000_toy.parquet` for posterity\n",
    "# First, we are creating few partition buckets\n",
    "df.write.partitionBy(\"_c0\", \"_c22\").parquet(directory_name)\n",
    "\n",
    "print(f'... completed job in {time.time() - start} seconds.')\n",
    "# ... completed job in 0.9980945587158203 seconds. 10000\n",
    "# ... completed job in 17.89224362373352 seconds.  101000\n",
    "# ... completed job in 11.039973020553589 seconds. 1mil\n",
    "# ... completed job in 24.255266427993774 seconds. 5 mil\n",
    "# ... completed job in 24.572181463241577 seconds. 10 mil\n",
    "# ... completed job in 41.63871169090271 seconds. full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
